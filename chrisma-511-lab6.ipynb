{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Logical Question Answering in English and Indic using LLM-\nbased Models.\ncompare how the models are performing\nsee how transformer models understand and resolve logic-based questions,\n\nchoosen types of inferenctial reasoning : Blood reation, direction based and cause-effect\nOptions ; Syllogism, direction based, blood realtion, number series, puzzle based, cause-effect or assumption based MCQ\n\nDataset used : AI4Bharat Logical Reasoning Dataset, IndicQA\nOptions : SQuAD v2, IndicQA Dataset, AI4Bharat Logical Reasoning Dataset\n\nModel selection : \ngeneral LLM(English) - FlanT5\nIndic language (hindi) - IndicBert\nMultilingual - XLM-R\n\nEvaluation matrix - \nExact match (EM) - Measures how many answers exactly match the ground truth.\nF1 Score - Considers partial matches (important if model predicts slightly different wording but logically correct answer).\n\nCustom score for logical reasoning\nModel confidence","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"your_huggingface_token\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"ai4bharat/MILU\", data_dir=\"English\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y tensorflow tensorflow-intel tensorflow-gpu\nimport os\nos.environ[\"USE_TF\"] = \"0\"\nos.environ[\"USE_FLAX\"] = \"0\"\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom datasets import load_dataset\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade transformers -q\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T07:06:14.871968Z","iopub.execute_input":"2025-07-31T07:06:14.872769Z","iopub.status.idle":"2025-07-31T07:06:31.279642Z","shell.execute_reply.started":"2025-07-31T07:06:14.872739Z","shell.execute_reply":"2025-07-31T07:06:31.279005Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ---------- Upgrade & Environment Fix ----------\n!pip install --upgrade transformers datasets scikit-learn -q\n!pip uninstall -y tensorflow tensorflow-intel tensorflow-gpu -q\n\nimport os\nos.environ[\"USE_TF\"] = \"0\"\nos.environ[\"USE_FLAX\"] = \"0\"\n\n# ---------- Imports ----------\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom datasets import load_dataset\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ---------- Load Dataset (LogiQA English MCQ) ----------\ndataset = load_dataset(\"lucasmccabe/logiqa\", split=\"test\")\ndataset = dataset.select(range(min(100, len(dataset))))  # For quick test\n\n# ---------- Load Flan-T5 ----------\nmodel_name = \"google/flan-t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n\n# ---------- Answer Generation ----------\ndef generate_answer(prompt_text):\n    inputs = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=16, num_beams=4)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n\n# ---------- Prediction Loop ----------\npredictions, gold_labels, questions, options_list, raw_outputs = [], [], [], [], []\n\nfor ex in dataset:\n    context = ex[\"context\"]\n    question = ex[\"query\"]\n    opts = ex[\"options\"]\n    correct = ex[\"correct_option\"]\n\n    # Prompt for Flan‑T5\n    prompt = f\"Context: {context}\\nQuestion: {question}\\nOptions:\\n\"\n    for idx, opt in enumerate(opts):\n        prompt += f\"{chr(65+idx)}. {opt}\\n\"\n    prompt += \"Answer:\"\n\n    answer = generate_answer(prompt)\n    answer_upper = answer.upper().strip()\n    pred_label = next((chr(65+i) for i in range(len(opts)) if answer_upper.startswith(chr(65+i))), \"A\")\n\n    predictions.append(pred_label)\n    gold_labels.append(chr(65 + correct))\n    questions.append(question)\n    options_list.append(\", \".join(opts))\n    raw_outputs.append(answer)\n\n\n\n# ---------- Metrics ----------\naccuracy = accuracy_score(gold_labels, predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(gold_labels, predictions, average=\"weighted\")\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# ---------- Save to CSV ----------\ndf = pd.DataFrame({\n    \"Question\": questions,\n    \"Options\": options_list,\n    \"TrueAnswer\": gold_labels,\n    \"PredictedLabel\": predictions,\n    \"RawAnswer\": raw_outputs\n})\ndf.to_csv(\"flan_t5_logiqa_predictions.csv\", index=False)\nprint(\"Saved predictions to flan_t5_logiqa_predictions.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T07:10:24.299936Z","iopub.execute_input":"2025-07-31T07:10:24.300756Z","iopub.status.idle":"2025-07-31T07:10:39.505417Z","shell.execute_reply.started":"2025-07-31T07:10:24.300726Z","shell.execute_reply":"2025-07-31T07:10:39.504657Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping tensorflow-intel as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"Using the latest cached version of the dataset since lucasmccabe/logiqa couldn't be found on the Hugging Face Hub\nFound the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/lucasmccabe___logiqa/default/0.0.0/3c19b0488d794d30c36f73d132d8a22e64f42f2e (last modified on Thu Jul 31 07:00:41 2025).\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.2800\nPrecision: 0.3137\nRecall: 0.2800\nF1 Score: 0.2904\nSaved predictions to flan_t5_logiqa_predictions.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(dataset.column_names)\nprint(dataset[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T07:09:44.211662Z","iopub.execute_input":"2025-07-31T07:09:44.212316Z","iopub.status.idle":"2025-07-31T07:09:44.216956Z","shell.execute_reply.started":"2025-07-31T07:09:44.212287Z","shell.execute_reply":"2025-07-31T07:09:44.216068Z"}},"outputs":[{"name":"stdout","text":"['context', 'query', 'options', 'correct_option']\n{'context': 'In the planning of a new district in a township, it was decided to build a special community in the southeast, northwest, centered on the citizen park. These four communities are designated as cultural area, leisure area, commercial area and administrative service area. It is known that the administrative service area is southwest of the cultural area, and the cultural area is southeast of the leisure area.', 'query': 'Based on the above statement, which of the following can be derived?', 'options': ['Civic Park is north of the administrative service area.', 'The leisure area is southwest of the cultural area.', 'The cultural district is in the northeast of the business district.', 'The business district is southeast of the leisure area.'], 'correct_option': 0}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ---------------- Install Required ----------------\n!pip install transformers accelerate -q\n\n# ---------------- Imports ----------------\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport shutil\n\n# ---------------- Load Falcon ----------------\nmodel_name = \"tiiuae/falcon-7b-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# ---------------- Inference Function ----------------\ndef generate_falcon_answer(context, question, options):\n    prompt = f\"\"\"\nYou are a reasoning assistant. Choose the best option only as A, B, C, or D.\n\nContext: {context}\nQuestion: {question}\nOptions:\nA. {options[0]}\nB. {options[1]}\nC. {options[2]}\nD. {options[3]}\n\nAnswer with only A, B, C, or D.\n\"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=20)\n    raw_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    for ch in [\"A\", \"B\", \"C\", \"D\"]:\n        if ch in raw_answer:\n            return ch, raw_answer\n    return \"A\", raw_answer  # fallback\n\n# ---------------- Dataset (Already Loaded for Flan‑T5) ----------------\npredictions, gold_labels, questions, options_list, raw_outputs = [], [], [], [], []\n\nfor ex in dataset:  # dataset you already loaded for flan‑t5\n    context = ex[\"context\"]\n    question = ex[\"query\"]\n    opts = ex[\"options\"]\n    correct = ex[\"correct_option\"]\n\n    pred, raw = generate_falcon_answer(context, question, opts)\n\n    predictions.append(pred)\n    gold_labels.append(chr(65 + correct))\n    questions.append(question)\n    options_list.append(\", \".join(opts))\n    raw_outputs.append(raw)\n\n# ---------------- Metrics ----------------\naccuracy = accuracy_score(gold_labels, predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(gold_labels, predictions, average=\"weighted\")\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# ---------------- Save Predictions ----------------\ndf = pd.DataFrame({\n    \"Question\": questions,\n    \"Options\": options_list,\n    \"TrueAnswer\": gold_labels,\n    \"PredictedLabel\": predictions,\n    \"RawAnswer\": raw_outputs\n})\ndf.to_csv(\"falcon7b_logiqa_predictions.csv\", index=False)\nprint(\"Predictions saved to falcon7b_logiqa_predictions.csv\")\n\n# ---------------- Save Model Locally for Frontend ----------------\nshutil.make_archive(\"falcon_7b_instruct\", 'zip', model_name)\nprint(\"Model archived as falcon_7b_instruct.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T07:21:04.393252Z","iopub.execute_input":"2025-07-31T07:21:04.393999Z","iopub.status.idle":"2025-07-31T07:24:08.851715Z","shell.execute_reply.started":"2025-07-31T07:21:04.393966Z","shell.execute_reply":"2025-07-31T07:24:08.850627Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeeeb9d4c9d344bab24339c91f54bf69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5df79d463a354c2b960fe40f91fb7c36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2e4a214592246b58c36efe30b6a0607"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c127e5bbc1904746be9c2f50a546bfcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"970d04cd77e54a6899b51bf5328408cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb25ef75ce54d438ac5ee5d510b72ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9bed571973e436e814c415e14635e75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bc25bf6afd9478f89526f40c09f5dee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94e55c1d6ada4fd5b3c93b47c129f308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b6a79a8d5034308a50b0eb2a2f4fd75"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.2200\nPrecision: 0.0484\nRecall: 0.2200\nF1 Score: 0.0793\nPredictions saved to falcon7b_logiqa_predictions.csv\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  y_true : 1d array-like, or label indicator array / sparse matrix\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_158/2982594172.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# ---------------- Save Model Locally for Frontend ----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"falcon_7b_instruct\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model archived as falcon_7b_instruct.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0msave_cwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mstmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_ISDIR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotADirectoryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOTDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Not a directory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tiiuae/falcon-7b-instruct'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'tiiuae/falcon-7b-instruct'","output_type":"error"}],"execution_count":9},{"cell_type":"markdown","source":"Deepseek","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\nimport gc\n\n# ---- Device ----\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---- English Model (Flan-T5-Large) ----\nenglish_model_name = \"google/flan-t5-base\"\nenglish_tokenizer = AutoTokenizer.from_pretrained(english_model_name)\nenglish_model = AutoModelForSeq2SeqLM.from_pretrained(\n    english_model_name,\n    torch_dtype=torch.float16\n).to(device)\n\n# ---- Metrics ----\ndef exact_match(pred, gold):\n    return int(pred.strip().lower() == gold.strip().lower())\n\ndef compute_f1(pred, gold):\n    pred_tokens, gold_tokens = pred.split(), gold.split()\n    common = set(pred_tokens) & set(gold_tokens)\n    if not common:\n        return 0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gold_tokens)\n    return 2 * (precision * recall) / (precision + recall)\n\ndef model_confidence(logits):\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    max_probs = probs.max(dim=-1).values\n    return max_probs.mean().item()\n\n# ---- Evaluate ----\ndef evaluate_flan_t5(dataset):\n    results = []\n    for _, row in dataset.iterrows():\n        question, answer = row[\"question\"], row[\"answer\"]\n\n        inputs = english_tokenizer(question, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = english_model.generate(\n                **inputs, \n                max_length=64,\n                output_scores=True,\n                return_dict_in_generate=True\n            )\n        pred = english_tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n        logits = torch.stack(outputs.scores, dim=1).squeeze(0)\n        confidence = model_confidence(logits)\n\n        em = exact_match(pred, answer)\n        f1 = compute_f1(pred, answer)\n        adjusted_conf = confidence if em == 1 else confidence * 0.5\n\n        results.append({\n            \"question\": question,\n            \"answer\": answer,\n            \"prediction\": pred,\n            \"EM\": em,\n            \"F1\": f1,\n            \"Confidence\": adjusted_conf\n        })\n    return results\n\n# ---- Example dataset ----\ndataset = pd.DataFrame({\n    \"question\": [\n        \"A is father of B. Who is B to A?\",\n        \"X moves North, then East. Which direction is X from starting point?\"\n    ],\n    \"answer\": [\"Son\", \"North-East\"]\n})\n\n# ---- Run Flan-T5-Large ----\nres_english = evaluate_flan_t5(dataset)\nprint(\"\\n--- English Model (Flan-T5-Large) ---\")\nfor r in res_english:\n    print(r)\n\n# ---- Clean GPU memory ----\ndel english_model\ntorch.cuda.empty_cache()\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T05:09:05.787362Z","iopub.execute_input":"2025-08-04T05:09:05.787632Z","iopub.status.idle":"2025-08-04T05:09:14.792742Z","shell.execute_reply.started":"2025-08-04T05:09:05.787612Z","shell.execute_reply":"2025-08-04T05:09:14.792166Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\n--- English Model (Flan-T5-Large) ---\n{'question': 'A is father of B. Who is B to A?', 'answer': 'Son', 'prediction': 'a', 'EM': 0, 'F1': 0, 'Confidence': 0.08894210308790207}\n{'question': 'X moves North, then East. Which direction is X from starting point?', 'answer': 'North-East', 'prediction': 'east', 'EM': 0, 'F1': 0, 'Confidence': 0.2898373305797577}\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"156"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\nimport gc\n\n# ---- Device ----\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---- English Model (Flan-T5-Base for safety) ----\nmodel_name = \"google/flan-t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16\n).to(device)\n\n# ---- Metrics ----\ndef exact_match(pred, gold):\n    return int(pred.strip().lower() == gold.strip().lower())\n\ndef compute_f1(pred, gold):\n    pred_tokens, gold_tokens = pred.split(), gold.split()\n    common = set(pred_tokens) & set(gold_tokens)\n    if not common:\n        return 0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gold_tokens)\n    return 2 * (precision * recall) / (precision + recall)\n\ndef model_confidence(logits):\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    max_probs = probs.max(dim=-1).values\n    return max_probs.mean().item()\n\n# ---- Evaluate ----\ndef evaluate_model(dataset, base_prompt=\"\", few_shot=False):\n    results = []\n    few_shot_examples = \"\"\"You are an expert in logical reasoning.\nExamples:\nQ: A is father of B. Who is B to A?\nA: Son\nQ: X moves North, then East. Which direction is X from starting point?\nA: North-East\n\"\"\"\n\n    for _, row in dataset.iterrows():\n        question, answer = row[\"question\"], row[\"answer\"]\n\n        if few_shot:\n            prompt = f\"{few_shot_examples}\\nNow answer:\\nQ: {question}\\nA:\"\n        else:\n            prompt = f\"{base_prompt}\\nQuestion: {question}\\nAnswer:\"\n\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs, \n                max_length=64,\n                output_scores=True,\n                return_dict_in_generate=True\n            )\n        pred = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n        logits = torch.stack(outputs.scores, dim=1).squeeze(0)\n        confidence = model_confidence(logits)\n\n        em = exact_match(pred, answer)\n        f1 = compute_f1(pred, answer)\n        adjusted_conf = confidence if em == 1 else confidence * 0.5\n\n        results.append({\n            \"question\": question,\n            \"answer\": answer,\n            \"prediction\": pred,\n            \"EM\": em,\n            \"F1\": f1,\n            \"Confidence\": adjusted_conf\n        })\n    return results\n\n# ---- Aggregated Evaluation ----\ndef evaluation_matrix(results):\n    em = sum(r[\"EM\"] for r in results) / len(results)\n    f1 = sum(r[\"F1\"] for r in results) / len(results)\n    conf = sum(r[\"Confidence\"] for r in results) / len(results)\n    return {\"Exact Match\": em, \"F1 Score\": f1, \"Confidence\": conf}\n\n# ---- Example dataset ----\ndataset = pd.DataFrame({\n    \"question\": [\n        \"A is father of B. Who is B to A?\",\n        \"X moves North, then East. Which direction is X from starting point?\"\n    ],\n    \"answer\": [\"Son\", \"North-East\"]\n})\n\n# ---- User input for custom prompt ----\nprint(\"\\nEnter your custom base prompt (press Enter for default):\")\nuser_prompt = input().strip()\nif not user_prompt:\n    user_prompt = \"You are an expert in logical reasoning. Think step by step and give the correct answer.\"\n\nprint(\"Do you want to use few-shot examples? (y/n):\")\nuse_few_shot = input().strip().lower() == \"y\"\n\n# ---- Run evaluation ----\nresults = evaluate_model(dataset, base_prompt=user_prompt, few_shot=use_few_shot)\n\n# ---- Print results ----\nprint(\"\\n--- Detailed Predictions ---\")\nfor r in results:\n    print(r)\n\nprint(\"\\n--- Evaluation Matrix ---\")\nmatrix = evaluation_matrix(results)\nprint(matrix)\n\n# ---- Clean up GPU memory ----\ndel model\ntorch.cuda.empty_cache()\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T05:13:58.963981Z","iopub.execute_input":"2025-08-04T05:13:58.964601Z","iopub.status.idle":"2025-08-04T05:14:24.762376Z","shell.execute_reply.started":"2025-08-04T05:13:58.964578Z","shell.execute_reply":"2025-08-04T05:14:24.761825Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nEnter your custom base prompt (press Enter for default):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" if ram is the father of sam, then what is sam to ram\n"},{"name":"stdout","text":"Do you want to use few-shot examples? (y/n):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" y\n"},{"name":"stdout","text":"\n--- Detailed Predictions ---\n{'question': 'A is father of B. Who is B to A?', 'answer': 'Son', 'prediction': 'Son', 'EM': 1, 'F1': 1.0, 'Confidence': 0.8456429243087769}\n{'question': 'X moves North, then East. Which direction is X from starting point?', 'answer': 'North-East', 'prediction': 'North-East', 'EM': 1, 'F1': 1.0, 'Confidence': 0.9072210788726807}\n\n--- Evaluation Matrix ---\n{'Exact Match': 1.0, 'F1 Score': 1.0, 'Confidence': 0.8764320015907288}\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"156"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport pandas as pd\nimport gc\n\n# ---- Device ----\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ---- Model ----\nmodel_name = \"google/flan-t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16\n).to(device)\n\n# ---- Metrics ----\ndef exact_match(pred, gold):\n    return int(pred.strip().lower() == gold.strip().lower())\n\ndef compute_f1(pred, gold):\n    pred_tokens, gold_tokens = pred.split(), gold.split()\n    common = set(pred_tokens) & set(gold_tokens)\n    if not common:\n        return 0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gold_tokens)\n    return 2 * (precision * recall) / (precision + recall)\n\ndef model_confidence(logits):\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    max_probs = probs.max(dim=-1).values\n    return max_probs.mean().item()\n\n# ---- Evaluate dataset ----\ndef evaluate_model(dataset, base_prompt=\"\", few_shot=False):\n    few_shot_examples = \"\"\"You are an expert in logical reasoning.\nExamples:\nQ: A is father of B. Who is B to A?\nA: Son\nQ: X moves North, then East. Which direction is X from starting point?\nA: North-East\n\"\"\"\n\n    results = []\n    for _, row in dataset.iterrows():\n        question, answer = row[\"question\"], row[\"answer\"]\n\n        prompt = (f\"{few_shot_examples}\\nNow answer:\\nQ: {question}\\nA:\"\n                  if few_shot else\n                  f\"{base_prompt}\\nQuestion: {question}\\nAnswer:\")\n\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_length=64,\n                output_scores=True,\n                return_dict_in_generate=True\n            )\n        pred = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n        logits = torch.stack(outputs.scores, dim=1).squeeze(0)\n        confidence = model_confidence(logits)\n\n        em = exact_match(pred, answer)\n        f1 = compute_f1(pred, answer)\n        adjusted_conf = confidence if em == 1 else confidence * 0.5\n\n        results.append({\n            \"question\": question,\n            \"answer\": answer,\n            \"prediction\": pred,\n            \"EM\": em,\n            \"F1\": f1,\n            \"Confidence\": adjusted_conf\n        })\n    return results\n\ndef evaluation_matrix(results):\n    em = sum(r[\"EM\"] for r in results) / len(results)\n    f1 = sum(r[\"F1\"] for r in results) / len(results)\n    conf = sum(r[\"Confidence\"] for r in results) / len(results)\n    return {\"Exact Match\": em, \"F1 Score\": f1, \"Confidence\": conf}\n\n# ---- Dataset ----\ndataset = pd.DataFrame({\n    \"question\": [\n        \"A is father of B. Who is B to A?\",\n        \"X moves North, then East. Which direction is X from starting point?\"\n    ],\n    \"answer\": [\"Son\", \"North-East\"]\n})\n\n# ---- User Prompt ----\nprint(\"\\nEnter your custom base prompt (press Enter for default):\")\nuser_prompt = input().strip()\nif not user_prompt:\n    user_prompt = \"You are an expert in logical reasoning. Think step by step and give the correct answer.\"\n\nprint(\"Do you want to use few-shot examples? (y/n):\")\nuse_few_shot = input().strip().lower() == \"y\"\n\n# ---- Evaluate dataset ----\nresults = evaluate_model(dataset, base_prompt=user_prompt, few_shot=use_few_shot)\nprint(\"\\n--- Dataset Predictions ---\")\nfor r in results:\n    print(r)\nprint(\"\\n--- Evaluation Matrix ---\")\nprint(evaluation_matrix(results))\n\n# ---- Interactive Question Mode ----\nwhile True:\n    user_question = input(\"\\nEnter your own logical question (or type 'exit'): \")\n    if user_question.lower() == \"exit\":\n        break\n    prompt = (f\"You are an expert in logical reasoning.\\n{user_prompt}\\n\"\n              f\"Question: {user_question}\\nAnswer:\")\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=64)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Answer: {answer}\")\n\n# ---- Cleanup ----\ndel model\ntorch.cuda.empty_cache()\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T05:16:53.209488Z","iopub.execute_input":"2025-08-04T05:16:53.210109Z","iopub.status.idle":"2025-08-04T05:41:45.795242Z","shell.execute_reply.started":"2025-08-04T05:16:53.210083Z","shell.execute_reply":"2025-08-04T05:41:45.794619Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nEnter your custom base prompt (press Enter for default):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" if neha is the mother of sneha then what is sneha to neha\n"},{"name":"stdout","text":"Do you want to use few-shot examples? (y/n):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" y\n"},{"name":"stdout","text":"\n--- Dataset Predictions ---\n{'question': 'A is father of B. Who is B to A?', 'answer': 'Son', 'prediction': 'Son', 'EM': 1, 'F1': 1.0, 'Confidence': 0.8456429243087769}\n{'question': 'X moves North, then East. Which direction is X from starting point?', 'answer': 'North-East', 'prediction': 'North-East', 'EM': 1, 'F1': 1.0, 'Confidence': 0.9072210788726807}\n\n--- Evaluation Matrix ---\n{'Exact Match': 1.0, 'F1 Score': 1.0, 'Confidence': 0.8764320015907288}\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter your own logical question (or type 'exit'):   if neha is the mother of sneha then what is sneha to neha\n"},{"name":"stdout","text":"Answer: neha is the mother of sneha\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter your own logical question (or type 'exit'):  Rahul starts from his house and walks 4 km towards the North, then turns right and walks 3 km. In which direction is he from his starting point?\n"},{"name":"stdout","text":"Answer: North\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter your own logical question (or type 'exit'):  exit\n"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"246"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"google/flan-t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\nsave_path = \"/kaggle/working/flan-t5-base\"\ntokenizer.save_pretrained(save_path)\nmodel.save_pretrained(save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T05:46:19.406741Z","iopub.execute_input":"2025-08-04T05:46:19.407289Z","iopub.status.idle":"2025-08-04T05:46:22.977287Z","shell.execute_reply.started":"2025-08-04T05:46:19.407265Z","shell.execute_reply":"2025-08-04T05:46:22.976489Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"google/flan-t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\nsave_path = \"/kaggle/working/flan-t5-base\"\ntokenizer.save_pretrained(save_path)\nmodel.save_pretrained(save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T05:49:00.109008Z","iopub.execute_input":"2025-08-04T05:49:00.109285Z","iopub.status.idle":"2025-08-04T05:49:03.763639Z","shell.execute_reply.started":"2025-08-04T05:49:00.109263Z","shell.execute_reply":"2025-08-04T05:49:03.762876Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/flan-t5-base\", 'zip', \"/kaggle/working/flan-t5-base\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T05:50:04.226607Z","iopub.execute_input":"2025-08-04T05:50:04.226920Z","iopub.status.idle":"2025-08-04T05:50:52.735552Z","shell.execute_reply.started":"2025-08-04T05:50:04.226889Z","shell.execute_reply":"2025-08-04T05:50:52.734984Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/flan-t5-base.zip'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"google/flan-t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\nsave_path = \"/kaggle/working/flan-t5-base\"\ntokenizer.save_pretrained(save_path)\nmodel.save_pretrained(save_path)\nimport shutil\nshutil.make_archive(\"/kaggle/working/flan-t5-base\", 'zip', save_path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T05:53:31.227773Z","iopub.execute_input":"2025-08-04T05:53:31.228342Z","iopub.status.idle":"2025-08-04T05:54:23.789098Z","shell.execute_reply.started":"2025-08-04T05:53:31.228318Z","shell.execute_reply":"2025-08-04T05:54:23.788487Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/flan-t5-base.zip'"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"Indiclanguage","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\n# IndicBERT QA fine-tuned model (if you have one)\nmodel_name = \"ai4bharat/indic-bert\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\n\nsave_path = \"/kaggle/working/indic-bert\"\ntokenizer.save_pretrained(save_path)\nmodel.save_pretrained(save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:18:42.371768Z","iopub.execute_input":"2025-08-05T00:18:42.372608Z","iopub.status.idle":"2025-08-05T00:19:24.998413Z","shell.execute_reply.started":"2025-08-05T00:18:42.372570Z","shell.execute_reply":"2025-08-05T00:19:24.997613Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3896ecb5f7a9484a95805b1cccd8be07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72e45aef07aa4bbf9fcb7568b802987d"}},"metadata":{}},{"name":"stderr","text":"2025-08-05 00:19:07.655173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754353148.023618      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754353148.125924      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ece7bfb71d4d9dae5fb046b34481ec"}},"metadata":{}},{"name":"stderr","text":"Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/indic-bert\", 'zip', save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:19:24.999464Z","iopub.execute_input":"2025-08-05T00:19:25.000034Z","iopub.status.idle":"2025-08-05T00:19:32.519513Z","shell.execute_reply.started":"2025-08-05T00:19:25.000015Z","shell.execute_reply":"2025-08-05T00:19:32.518737Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e9416a40fcf4bfcb251fdf83ba5c654"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/indic-bert.zip'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\nimport torch\n\nmodel_name = \"deepset/xlm-roberta-base-squad2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name).to(\"cpu\")\n\ncontext = \"राम सीता के पति हैं।\"\nquestion = \"सीता के पति कौन हैं?\"\n\ninputs = tokenizer(question, context, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nstart_idx = torch.argmax(outputs.start_logits)\nend_idx = torch.argmax(outputs.end_logits)\n\n# Handle case where model predicts no valid answer\nif start_idx == 0 and end_idx == 0:\n    answer = \"No answer found\"\nelse:\n    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx+1])\n    # Remove special tokens\n    tokens = [t for t in tokens if not t.startswith(\"<\")]\n    answer = tokenizer.convert_tokens_to_string(tokens)\n\nprint(\"Answer:\", answer)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:05:31.911813Z","iopub.execute_input":"2025-08-04T14:05:31.912103Z","iopub.status.idle":"2025-08-04T14:05:37.024381Z","shell.execute_reply.started":"2025-08-04T14:05:31.912081Z","shell.execute_reply":"2025-08-04T14:05:37.023739Z"}},"outputs":[{"name":"stdout","text":"Answer: No answer found\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install transformers sentencepiece\n\nfrom transformers import MarianTokenizer, MarianMTModel, AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Load translation models once\ndef load_translation_models():\n    hi_en_name = \"Helsinki-NLP/opus-mt-hi-en\"\n    en_hi_name = \"Helsinki-NLP/opus-mt-en-hi\"\n    hi_en_tokenizer = MarianTokenizer.from_pretrained(hi_en_name)\n    hi_en_model = MarianMTModel.from_pretrained(hi_en_name)\n    en_hi_tokenizer = MarianTokenizer.from_pretrained(en_hi_name)\n    en_hi_model = MarianMTModel.from_pretrained(en_hi_name)\n    return (hi_en_tokenizer, hi_en_model, en_hi_tokenizer, en_hi_model)\n\nhi_en_tokenizer, hi_en_model, en_hi_tokenizer, en_hi_model = load_translation_models()\n\ndef translate_hi_to_en(text):\n    tokens = hi_en_tokenizer(text, return_tensors=\"pt\", truncation=True)\n    translated = hi_en_model.generate(**tokens)\n    return hi_en_tokenizer.decode(translated[0], skip_special_tokens=True)\n\ndef translate_en_to_hi(text):\n    tokens = en_hi_tokenizer(text, return_tensors=\"pt\", truncation=True)\n    translated = en_hi_model.generate(**tokens)\n    return en_hi_tokenizer.decode(translated[0], skip_special_tokens=True)\n\n# --- Example usage ---\nhindi_text = \"राम सीता के पति हैं।\"\nenglish_text = translate_hi_to_en(hindi_text)\nprint(\"Hindi → English:\", english_text)\n\nhindi_back = translate_en_to_hi(\"Ram is Sita's husband.\")\nprint(\"English → Hindi:\", hindi_back)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:09:11.142492Z","iopub.execute_input":"2025-08-04T14:09:11.143062Z","iopub.status.idle":"2025-08-04T14:09:28.146480Z","shell.execute_reply.started":"2025-08-04T14:09:11.143039Z","shell.execute_reply":"2025-08-04T14:09:28.145361Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7db2aaf169a4be8813c92fa02285b07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63baec889b4d4f1187c4aa0014a8491c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/813k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bab957d3591417da9d8903d59572c23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dca9af184cd94a8f8c613b18db4bfd48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"809d30ce14f54c17a712e389a0f2d1d7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:177: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/304M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89aaa6a6b3ce4f60ab5fb3fc24fb2f33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"839f4b5250b745508e42f5b267dcd158"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"881ac5cdd0e3448ab202eec6ed77c12e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/304M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3916cdb3cb6f4ce09b52e7d3b4d8f722"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/812k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc939bdc63c4983a979f88a16f499b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/1.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2df97aeb48542ab99e098c025f05a0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"916fdc59bcd74d8ebda091f202c8ccac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2bafb1acb794f78831bd60bde62694b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32917dda7e914bbcb150340ed06fb531"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edec47aa455a4cc185a85b354d3d80b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/306M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7230ef0fe874640bc888fa12ce2787b"}},"metadata":{}},{"name":"stdout","text":"Hindi → English: Ram Sita's husband.\nEnglish → Hindi: राम सीता के पति है.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\n# Load Hindi QA model\ndef load_hindi_qa_model():\n    model_name = \"deepset/xlm-roberta-base-squad2\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(\"cpu\")\n    return tokenizer, model\n\nhindi_tokenizer, hindi_model = load_hindi_qa_model()\n\ndef get_hindi_answer(question, context, threshold=0.2):\n    inputs = hindi_tokenizer(question, context, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = hindi_model(**inputs)\n\n    # Probabilities\n    start_probs = F.softmax(outputs.start_logits, dim=-1)\n    end_probs = F.softmax(outputs.end_logits, dim=-1)\n    start_idx = torch.argmax(outputs.start_logits)\n    end_idx = torch.argmax(outputs.end_logits)\n    confidence = (start_probs[0, start_idx] * end_probs[0, end_idx]).item()\n\n    if confidence < threshold or (start_idx == 0 and end_idx == 0):\n        return \"No answer found (low confidence)\", confidence\n\n    tokens = hindi_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx+1])\n    tokens = [t for t in tokens if not t.startswith(\"<\")]\n    answer = hindi_tokenizer.convert_tokens_to_string(tokens)\n    return answer, confidence\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:11:04.854559Z","iopub.execute_input":"2025-08-04T14:11:04.855250Z","iopub.status.idle":"2025-08-04T14:11:10.162709Z","shell.execute_reply.started":"2025-08-04T14:11:04.855224Z","shell.execute_reply":"2025-08-04T14:11:10.161921Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"answer, confidence = get_hindi_answer(question, context)\nif \"No answer found\" in answer:\n    # Translate question & context to English\n    question_en = translate_hi_to_en(question)\n    context_en = translate_hi_to_en(context)\n\n    # Run English QA model (Flan-T5 or any generative model)\n    inputs = flan_tokenizer(question_en, context_en, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = flan_model.generate(**inputs)\n    english_answer = flan_tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Translate back to Hindi\n    answer_hi = translate_en_to_hi(english_answer)\n    st.write(f\"**Fallback Answer (via translation):** {answer_hi}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:11:19.714933Z","iopub.execute_input":"2025-08-04T14:11:19.715270Z","iopub.status.idle":"2025-08-04T14:11:20.731730Z","shell.execute_reply.started":"2025-08-04T14:11:19.715246Z","shell.execute_reply":"2025-08-04T14:11:20.730699Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1007412111.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Run English QA model (Flan-T5 or any generative model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflan_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflan_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'flan_tokenizer' is not defined"],"ename":"NameError","evalue":"name 'flan_tokenizer' is not defined","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"# --- Load Hindi QA model (from earlier) ---\nhindi_tokenizer, hindi_model = load_hindi_qa_model()\n\n# --- Load English QA model ---\nflan_model_name = \"google/flan-t5-base\"\nflan_tokenizer = AutoTokenizer.from_pretrained(flan_model_name)\nflan_model = AutoModelForSeq2SeqLM.from_pretrained(flan_model_name).to(\"cpu\")\n\n# --- Hindi Question & Context ---\nquestion = \"सीता के पति कौन हैं?\"\ncontext = \"राम सीता के पति हैं।\"\n\n# --- Run Hindi QA ---\nanswer, confidence = get_hindi_answer(question, context)\n\nif \"No answer found\" in answer:\n    # Translate Hindi → English\n    question_en = translate_hi_to_en(question)\n    context_en = translate_hi_to_en(context)\n\n    # Run English QA (Flan-T5)\n    inputs = flan_tokenizer(question_en + \" Context: \" + context_en, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = flan_model.generate(**inputs)\n    english_answer = flan_tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Translate English answer → Hindi\n    answer = translate_en_to_hi(english_answer)\n\nprint(\"Final Answer:\", answer)\nprint(\"Confidence (Hindi model):\", confidence)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:12:36.658868Z","iopub.execute_input":"2025-08-04T14:12:36.659176Z","iopub.status.idle":"2025-08-04T14:12:50.187339Z","shell.execute_reply.started":"2025-08-04T14:12:36.659153Z","shell.execute_reply":"2025-08-04T14:12:50.186563Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f730d53a23e4a3e90796e867b519575"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4cba5e4d2f540b2afb9ba5f69e578f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bd0bb0869ce4f2699c94ff220802d18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9528aec4807d414fbd7afb3df854361f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"313a071aea7b4131bb72bd8e29b3886e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b672b543e724e1895b2bf66fb119423"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06551ff27e60448485399ec3a7b725d9"}},"metadata":{}},{"name":"stdout","text":"Final Answer: राम सीता\nConfidence (Hindi model): 0.8960077166557312\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def normalize_text(text):\n    import re\n    text = text.lower()\n    text = re.sub(r\"[^a-zA-Z0-9\\u0900-\\u097F\\s]\", \"\", text)  # keep Hindi + English\n    return \" \".join(text.split())\n\ndef compute_f1(pred, truth):\n    pred_tokens = normalize_text(pred).split()\n    truth_tokens = normalize_text(truth).split()\n    common = set(pred_tokens) & set(truth_tokens)\n    if len(common) == 0:\n        return 0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(truth_tokens)\n    return 2 * precision * recall / (precision + recall)\n\ndef evaluate_dataset(dataset):\n    results = []\n    total_em, total_f1, total_conf = 0, 0, 0\n\n    for data in dataset:\n        question = data[\"question\"]\n        context = data[\"context\"]\n        ground_truth = data[\"answer\"]\n\n        # Hindi QA\n        answer, confidence = get_hindi_answer(question, context)\n        if \"No answer found\" in answer:\n            # Fallback (English QA)\n            question_en = translate_hi_to_en(question)\n            context_en = translate_hi_to_en(context)\n            inputs = flan_tokenizer(question_en + \" Context: \" + context_en, return_tensors=\"pt\")\n            with torch.no_grad():\n                outputs = flan_model.generate(**inputs)\n            english_answer = flan_tokenizer.decode(outputs[0], skip_special_tokens=True)\n            answer = translate_en_to_hi(english_answer)\n\n        # Metrics\n        em = 1 if normalize_text(answer) == normalize_text(ground_truth) else 0\n        f1 = compute_f1(answer, ground_truth)\n\n        total_em += em\n        total_f1 += f1\n        total_conf += confidence\n\n        results.append({\n            \"question\": question,\n            \"answer\": ground_truth,\n            \"prediction\": answer,\n            \"EM\": em,\n            \"F1\": f1,\n            \"Confidence\": confidence\n        })\n\n    n = len(dataset)\n    matrix = {\n        \"Exact Match\": total_em / n,\n        \"F1 Score\": total_f1 / n,\n        \"Avg Confidence\": total_conf / n\n    }\n    return results, matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:14:09.871384Z","iopub.execute_input":"2025-08-04T14:14:09.872351Z","iopub.status.idle":"2025-08-04T14:14:09.880839Z","shell.execute_reply.started":"2025-08-04T14:14:09.872319Z","shell.execute_reply":"2025-08-04T14:14:09.880075Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Example Hindi dataset\ndataset = [\n    {\"question\": \"सीता के पति कौन हैं?\", \"context\": \"राम सीता के पति हैं।\", \"answer\": \"राम\"},\n    {\"question\": \"भारत की राजधानी क्या है?\", \"context\": \"भारत की राजधानी नई दिल्ली है।\", \"answer\": \"नई दिल्ली\"}\n]\n\nresults, matrix = evaluate_dataset(dataset)\nprint(\"--- Detailed Predictions ---\")\nfor r in results:\n    print(r)\nprint(\"\\n--- Evaluation Matrix ---\")\nprint(matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T14:14:19.134430Z","iopub.execute_input":"2025-08-04T14:14:19.134709Z","iopub.status.idle":"2025-08-04T14:14:20.588556Z","shell.execute_reply.started":"2025-08-04T14:14:19.134687Z","shell.execute_reply":"2025-08-04T14:14:20.587663Z"}},"outputs":[{"name":"stdout","text":"--- Detailed Predictions ---\n{'question': 'सीता के पति कौन हैं?', 'answer': 'राम', 'prediction': 'राम सीता', 'EM': 0, 'F1': 0.6666666666666666, 'Confidence': 0.8960077166557312}\n{'question': 'भारत की राजधानी क्या है?', 'answer': 'नई दिल्ली', 'prediction': 'नई दिल्ली', 'EM': 1, 'F1': 1.0, 'Confidence': 0.8035038113594055}\n\n--- Evaluation Matrix ---\n{'Exact Match': 0.5, 'F1 Score': 0.8333333333333333, 'Avg Confidence': 0.8497557640075684}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def evaluate_dataset_with_accuracy(dataset):\n    total_em, total_f1, total_conf, correct, results = 0, 0, 0, 0, []\n    for d in dataset:\n        ans, conf = get_indicbert_answer(d[\"question\"], d[\"context\"])\n        em = 1 if normalize_text(ans) == normalize_text(d[\"answer\"]) else 0\n        f1 = compute_f1(ans, d[\"answer\"])\n        total_em += em\n        total_f1 += f1\n        total_conf += conf\n        if em == 1:  # Count correct answers\n            correct += 1\n        results.append({\n            \"question\": d[\"question\"],\n            \"answer\": d[\"answer\"],\n            \"prediction\": ans,\n            \"EM\": em,\n            \"F1\": f1,\n            \"Confidence\": conf\n        })\n    \n    n = len(dataset)\n    metrics = {\n        \"Exact Match\": total_em/n,\n        \"F1 Score\": total_f1/n,\n        \"Avg Confidence\": total_conf/n,\n        \"Accuracy\": correct/n  # <--- Added Accuracy\n    }\n    return results, metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:13:35.585554Z","iopub.execute_input":"2025-08-05T00:13:35.585790Z","iopub.status.idle":"2025-08-05T00:13:35.596796Z","shell.execute_reply.started":"2025-08-05T00:13:35.585766Z","shell.execute_reply":"2025-08-05T00:13:35.596096Z"}},"outputs":[],"execution_count":1}]}